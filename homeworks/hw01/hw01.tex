\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 621 Fall 2017 Homework \#1}

\author{Professor Adam Kapelner} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due \emph{in class}, Thursday, September 14, 2017 \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, review Math 241 concerning random variables, support, parameter space, PMF's, CDF's. Then read about convolutions and the multinomial distribution.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}


\problem{These exercises review convolutions.}


\begin{enumerate}

\intermediatesubproblem{Let $X_1, X_2 \iid \bernoulli{p}$. Using the definition of convolution without the indicator function within $p_{X_2}$, write out all terms of the sum and identify which one is illegal.}\spc{4}

\easysubproblem{Let $X_1 \sim \bernoulli{p_1}$ independent of $X_2 \sim \bernoulli{p_2}$. Find the PMF of the sum of $T = X_1 + X_2$ using basic probability theory. You can denote the PMF using a piecewise function (with cases).}\spc{4}

\intermediatesubproblem{Prove the PMF of $T = X_1 + X_2$ from (b) using a convolution. Show that the function reduces to the piecewise function in (b) when substituting for the values of $t$.}\spc{6}


\hardsubproblem{Let 

\beqn
X_1 \sim \begin{cases}
3 \withprob 0.3 \\
6 \withprob 0.7
\end{cases} \quad \text{independent of} \quad
%
X_2 \sim \begin{cases}
4 \withprob 0.4 \\
8 \withprob 0.6
\end{cases} 
\eeqn

Find the PMF of $T = X_1 + X_2$ using a convolution.}\spc{6}


\hardsubproblem{Prove the PMF of a binomial inductively using convolutions on the sequence of r.v.'s $\Xoneton \iid \bernoulli{p}$. You will need to use Pascal's Triangle combinatorial identity we employed in class.}\spc{7}


\extracreditsubproblem{Prove the PMF of a negative binomial inductively using convolutions on the sequence of r.v.'s $\Xoneton \iid \geometric{p}$. You will need to use the \href{https://en.wikipedia.org/wiki/Hockey-stick_identity}{hockey stick identity}.}\spc{7}

\hardsubproblem{Let $X_1 \sim \binomial{n_1}{p}$ independent of $X_2 \sim \binomial{n_2}{p}$. Find the PMF of the sum of $T = X_1 + X_2$ using a convolution.}\spc{13}


\easysubproblem{Prove the PMF of $X \sim \poisson{\lambda}$ using the limit as $n \rightarrow \infty$ and let $p = \overn{\lambda}$.}\spc{9}


\hardsubproblem{Let $X_1 \sim \poisson{\lambda_1}$ independent of $X_2 \sim \poisson{\lambda_2}$. Find the PMF of the sum of $T = X_1 + X_2$ using a convolution.}\spc{9}



\end{enumerate}


\problem{These exercises introduce probabilities of conditional subsets of the supports of multiple r.v.'s.}


\begin{enumerate}

\hardsubproblem{Let $X \sim \geometric{p_x}$ independent of $Y \sim \geometric{p_y}$. Find $\prob{X > Y}$.}\spc{10}


\hardsubproblem{Given (a), find $\prob{X = Y}$.}\spc{10}

\hardsubproblem{As both $p_x$ and $p_y$ are reduced to zero, but $r = \frac{p_x}{p_y}$, what is the asymptotic probability you found in (a)?}\spc{10}


\easysubproblem{Without needing to compute (c), what is the answer if $r=1$?}\spc{0}

\intermediatesubproblem{Let $X \sim \poisson{\lambda}$ independent of $Y \sim \poisson{\lambda}$. Find an expression for $\prob{X > Y}$ \emph{as best as you are able to answer}. Part of this exercise is identifying where you cannot go any further.}\spc{9}


\end{enumerate}


\problem{These exercises will introduce the Multinomial distribution.}


\begin{enumerate}

\easysubproblem{If $\X \sim \multinomial{n}{\p}$ where $\dime{\X} = k$, what is $\dime{\p}$?}\spc{0}


\easysubproblem{If $\X \sim \multinomial{n}{\p}$ and $n= 10$ and $\dime{\X} = 15$ as a column vector, give an example value of $\x$, a realization of the r.v. $\X$.}\spc{2}


\intermediatesubproblem{If $\X \sim \multinomial{n}{\p}$ and $n= 10$ and $\p = \bracks{0.2, 0.8}^\top$, find $\muvec := \expe{\X}$.}\spc{2}


\hardsubproblem{If $\X_1 \sim \multinomial{n}{\p}$ and independently $\X_2 \sim \multinomial{n}{\p}$ where $\dime{\X_1} = \dime{\X_2} = k$. Find the JMF of $\T_2 = \X_1 + \X_2$ from the definition of convolution. This looks harder than it is! First, use the definition of convolution. Then follow 1(g) updating each piece of information from binomial to multinomial, Finally, use Theorem 1 in \href{http://www.lrecits.usthb.dz/1.3.pdf}{this paper} for the summation.}\spc{9}

\end{enumerate}


\end{document}


\problem{These exercises will review the Bernoulli model.}


\begin{enumerate}

\easysubproblem{If $X \sim \bernoulli{\theta}$, find $\expe{X}$, $\var{X}$, $\support{X}$ and $\Theta$. No need to derive from first principles, just find the formulas.}\spc{2}

\intermediatesubproblem{If $X \sim \bernoulli{\theta}$, find $\median{X}$.}\spc{2}

\intermediatesubproblem{If $X \sim \bernoulli{\theta}$, write the \qu{parametric statistical model} below using the notation we used in class only.}\spc{2}


\intermediatesubproblem{Explain what the semicolon notation in the previous answer indicates.}\spc{2}

\easysubproblem{If $\Xoneton \iid \bernoulli{\theta}$, find the likelihood, $\mathcal{L}$, of $\theta$.}\spc{2}

\hardsubproblem{Given the likelihood above, what would $\mathcal{L}$ be if the data was $<0,1,0,1,3.7>$? Why should this answer have to be?}\spc{2}

\easysubproblem{If $\Xoneton \iid \bernoulli{\theta}$, find the log-likelihood of $\theta$.}\spc{2}

\hardsubproblem{[MA] If $\Xoneton \iid f(x;\theta)$, explain why the log-likelihood of $\theta$ is normally distributed if $n$ gets large.}\spc{6}

\easysubproblem{If $\Xoneton \iid \bernoulli{\theta}$, find the score function (i.e the derivative of the log-likelihood) of $\theta$.}\spc{2}

\intermediatesubproblem{If $\Xoneton \iid \bernoulli{\theta}$, find the maximum likelihood estimator for $\theta$.}\spc{5}

\easysubproblem{If $\Xoneton \iid \bernoulli{\theta}$, find the maximum likelihood \textit{estimate} for $\theta$.}\spc{1}

\easysubproblem{Given the previous two questions, describe the difference between a random variable and a datum.}\spc{3}

\easysubproblem{If your data is $<0,1,1,0,1,1,0,1,1,1>$, find the maximum likelihood estimate for $\theta$.}\spc{1}

\easysubproblem{Given this data, find a 99\% confidence interval for $\theta$.}\spc{3}

\easysubproblem{Given this data, test $H_0: \theta = 0.5$ versus $H_a: \theta \neq 0.5$.}\spc{3}

\end{enumerate}

\problem{We will review the frequentist perspective here.}

\begin{enumerate}

\hardsubproblem{Why do frequentists have an insistence on $\theta$ being a fixed, immutable quantity? We didn't cover this in class explicitly but it is lurking behind the scenes.}\spc{3}

\easysubproblem{What are the three goals of inference?}\spc{4}

\easysubproblem{What are the three reasons why \emph{frequentists} (adherents to the frequentist perspective) use MLEs i.e. list three properties of MLEs that make them powerful.}\spc{3}

\hardsubproblem{[MA] Give the conditions for asymptotic normality of the MLE,

\beqn
\frac{\thetahatmle - \theta}{\se{\thetahatmle}} \convd \stdnormnot.
\eeqn

You can find them online.}\spc{8}

\hardsubproblem{[MA] $\se{\thetahatmle}$ cannot be found without $\theta$ so we substituted $\thetahatmle$ into $\se{\thetahatmle}$ and called it $\seest{\thetahatmle}$ (note the hat over the SE). Show that this too is asymptotically normal, \ie

\beqn
\frac{\thetahatmle - \theta}{\seest{\thetahatmle}} \convd \stdnormnot
\eeqn

You need the continuous mapping theorem and Slutsky's theorem.
}\spc{4}

\easysubproblem{[MA] Explain why the previous question allows us to build asymptotically valid confidence intervals using $\bracks{\thetahatmle \pm z_{\alpha/2} \seest{\thetahatmle}}$}.\spc{3}

\intermediatesubproblem{Why does all of frequentist inference break down if $n$ isn't large?}\spc{9}

\easysubproblem{Write the most popular two frequentist interpretations of a confidence interval.}\spc{6}

\intermediatesubproblem{Why are each of these unsatisfactory?}\spc{3}

\easysubproblem{What are the two possible outcomes of a hypothesis test?}\spc{3}

\hardsubproblem{[MA] What is the weakness of the interpretation of the $p$-val?}\spc{6}


\end{enumerate}


\problem{We review and build upon conditional probability here.}

\begin{enumerate}


\easysubproblem{Explain why $\cprob{B}{A} \propto \prob{A,B}$.}\spc{3}

\easysubproblem{If $B$ represents the hypothesis or the putative cause and $A$ represents evidence or data, explain what Bayesian Conditionalism is, going from which probability statement to which probability statement.}\spc{3}

\end{enumerate}


\end{document}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\intermediatesubproblem{In class we presented the posterior odds form of Bayes Theorem. Prove it below.}\spc{10}


\intermediatesubproblem{Show that the Bayes Factor is the ratio of posterior odds of the hypothesis to prior odds of the hypothesis.}\spc{2}

\easysubproblem{On the \href{https://en.wikipedia.org/wiki/Bayes_factor}{wikipedia page about Bayes Factors}, Harrold Jeffreys (who we will be returning to later in the semester) gave interpretations of Bayes Factors (which is denoted $K$ there and $B$ in Bolstad's book on page 70). Give the ranges of $K$ here (not in terms of powers of 10, but as a pure number) for his interpretations i.e. \qu{negative,} \qu{strong,} etc.}\spc{3}

\hardsubproblem{[MA] Conceptually why should the likelihood being greater than $\prob{A}$ imply that the hypothesis is more likely after observing the data than before?}\spc{6}
\end{enumerate}

\problem{We examine here paternity testing (i.e. answering the question \qu{is this guy the father of my child?}) via the simplistic test using blood types. These days, more advanced genetic methods exist so these calculations aren't made in practice, but they are a nice exercise. 

First a crash course on basic genetics. In general, everyone has two alleles (your genotype) with one coming from your mother and one coming from your father. The mother passes on each of the alleles with 50\% probability and the father passes on each allele with 50\% probability. One allele gets expressed (your phenotype). So one of the genes shone through (the dominant one) and one was masked (the recessive one). Dominant blood types are A and B and the recessive type is o (lowercase letter). The only way to express phenotype o is to have genotype oo i.e. both genes are o. There is an exception; A and B are codominant meaning that blood type AB tests positive for both A and B.

In this case consider a child of blood type B and the mother of blood type A. Using this \href{http://www.cccoe.net/genetics/blood2.html}{hereditary guide}, we know that the mother's type must be Ao so she passed on an o to the child thus the child got the B from the father. Thus the father had type AB, BB or Bo. I got the following data from \href{http://www.sciencedirect.com/science/article/pii/S1110863011000796}{this paper} (so let's assume this case is in Nigeria in 1998).

\begin{table}
\centering
\begin{tabular}{cc}
Genotype & Frequency \\ \hline
OO	&0.52 \\
AA	&0.0196 \\
AO	&0.2016 \\
BB	&0.0196 \\
BO	&0.2016 \\
AB	&0.0392 \\
\end{tabular}
\end{table}
} 

\begin{enumerate}

\easysubproblem{Bob is the alleged father and he has blood type B but his genotype is unknown. What is the probability he passes on a B to the child?}\spc{3}

\easysubproblem{What is the probability a stranger passes on a B to the child?}\spc{3}

\easysubproblem{Assume our prior is 50-50 Bob is the father, the customary compromise between a possibly bitter mother and father. What is the prior odds of Bob being the father? Don't think too hard about this one; it is marked easy for a reason.}\spc{6}

\hardsubproblem{We are interested in the posterior question. What is the probability Bob is the father given the child with blood type B?}\spc{5}

\hardsubproblem{What is the Bayes Factor here? See (a) and (b).}\spc{5}

\easysubproblem{What is the probability Bob is not the father given the child with blood type B? Should be easy once you have (c) and (e).}\spc{3}

\end{enumerate}


\end{document}